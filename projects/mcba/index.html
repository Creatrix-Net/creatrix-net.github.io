<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN" "http://www.w3.org/TR/REC-html40/loose.dtd"> <html><body> <h2 id="abstract">Abstract</h2> <p>Advancements in bionic technology are transforming the possibilities for restoring hand function in individuals with amputations or paralysis. This paper introduces a <strong>cost-effective bionic arm</strong> design that leverages <strong>mind-controlled functionality</strong> and integrates a <strong>sense of touch</strong> to replicate natural hand movements. The system utilizes a <strong>non-invasive EEG-based control mechanism</strong>, enabling users to operate the arm using brain signals processed into PWM commands for servo motor control of the bionic arm. Additionally, the design incorporates a touch sensor (tactile feedback) in the gripper, offering sensory feedback to enhance user safety and dexterity. The proposed bionic arm prioritizes three essential features:</p> <ol> <li> <strong>Integrated Sensory Feedback</strong>: Providing users with a tactile experience to mimic the sense of touch (signals directly going to the brain). This capability is crucial for safe object manipulation by arm and preventing injuries</li> <li> <strong>Mind-Control Potential</strong>: Harnessing EEG signals for seamless, thought-driven operation.</li> <li> <strong>Non-Invasive Nature</strong>: Ensuring user comfort by avoiding invasive surgical procedures. This novel approach aims to deliver an intuitive, natural, and efficient solution for restoring complex hand functions.</li> </ol> <hr> <h2 id="methodology">Methodology</h2> <h3 id="1-data-collection-and-dataset-overview">1. Data Collection and Dataset Overview</h3> <p>The model development utilized a publicly available EEG dataset comprising data from <strong>60 volunteers</strong> performing <strong>8 distinct activities</strong> [3]. The dataset includes a total of <strong>8,680 four-second EEG recordings</strong>, collected using <strong>16 dry electrodes</strong> configured according to the <strong>international 10-10 system</strong> [3].</p> <ul> <li>Electrode Configuration: Monopolar configuration, where each electrode’s potential was measured relative to neutral electrodes placed on both earlobes (ground references).</li> <li>Signal Sampling: EEG signals were sampled at <strong>125 Hz</strong> and preprocessed using: <ul> <li> <strong>A bandpass filter (5–50 Hz)</strong> to isolate relevant frequencies [3].</li> <li> <strong>A notch filter (60 Hz)</strong> to remove powerline interference [3].</li> </ul> </li> </ul> <h3 id="2-data-preprocessing">2. Data Preprocessing</h3> <p>The dataset, originally provided in <strong>CSV format</strong>, underwent a comprehensive preprocessing workflow:</p> <ul> <li>The data was split into individual CSV files for each of the 16 channels, resulting in an increase from <strong>74,441</strong> files to <strong>1,191,056</strong> files.</li> <li>Each individual channel’s EEG data was converted into <strong>audio signals</strong> and saved in <strong>.wav format</strong>, allowing the brain signals to be audibly analyzed.</li> <li>The entire preprocessing workflow was implemented in <strong>Python</strong> to ensure scalability and accuracy. The dataset captured brainwave signals corresponding to the following activities: <ol> <li> <strong>BEO</strong> (Baseline with Eyes Open): One-time recording at the beginning of each run [3].</li> <li> <strong>CLH</strong> (Closing Left Hand): Five recordings per run [3].</li> <li> <strong>CRH</strong> (Closing Right Hand): Five recordings per run [3].</li> <li> <strong>DLF</strong> (Dorsal Flexion of Left Foot): Five recordings per run [3].</li> <li> <strong>PLF</strong> (Plantar Flexion of Left Foot): Five recordings per run [3].</li> <li> <strong>DRF</strong> (Dorsal Flexion of Right Foot): Five recordings per run [3].</li> <li> <strong>PRF</strong> (Plantar Flexion of Right Foot): Five recordings per run [3].</li> <li> <strong>Rest</strong>: Recorded between each task to capture the resting state [3] [4].</li> </ol> </li> </ul> <h3 id="3-feature-extraction-and-classification">3. Feature Extraction and Classification</h3> <p>Feature extraction and activity classification were performed using <strong>transfer learning</strong> with <strong>YamNet</strong> <d-cite key="yamnet_github"></d-cite>, a deep neural network model.</p> <ul> <li> <strong>Audio Representation</strong>: Audio files were imported into <strong>MATLAB</strong> using an <strong>Audio Datastore</strong> [6]. Mel-spectrograms, a time-frequency representation of the audio signals, were extracted using the yamnetPreprocess <d-cite key="yamnetpreprocess"></d-cite> function <d-cite key="transferlearning_matlab"></d-cite>.</li> <li>Dataset Split: The data was divided into <strong>training (70%)</strong>, <strong>validation (20%)</strong>, and <strong>testing (10%)</strong> sets. Transfer Learning with YamNet <d-cite key="yamnet_github"></d-cite> <d-cite key="transferlearning_matlab"></d-cite>:</li> <li>The <strong>pre-trained YamNet model</strong> (86 layers) <d-cite key="yamnet_github"></d-cite> was adapted for an 8-class classification task: <ul> <li>The initial layers of YamNet <d-cite key="yamnet_github"></d-cite> were <strong>frozen</strong> to retain previously learned representations <d-cite key="transferlearning_matlab"></d-cite>.</li> <li>A <strong>new classification layer</strong> was added to the model <d-cite key="transferlearning_matlab"></d-cite>.</li> </ul> </li> <li>Training details: <ul> <li> <strong>Learning Rate</strong>: Initial rate of <strong>3e-4</strong>, with an exponential learning rate decay schedule <d-cite key="transferlearning_matlab"></d-cite>.</li> <li> <strong>Mini-Batch Size</strong>: 128 samples per batch.</li> <li> <strong>Validation</strong>: Performed every <strong>651 iterations</strong>.</li> </ul> </li> </ul> <h3 id="4-robotic-arm-design-and-simulation">4. Robotic Arm Design and Simulation</h3> <p>A <strong>3-Degree-of-Freedom (DOF) robotic arm</strong> was designed using <strong>MATLAB Simulink</strong> and <strong>Simscape toolboxes</strong>. To ensure robust validation:</p> <ul> <li>A <strong>virtual environment</strong> was developed in Simulink, simulating the interactions between the trained AI models and the robotic arm.</li> <li>The simulations served as a testbed to evaluate the system’s performance before real-world integration.</li> </ul> <h3 id="5-project-progress-and-future-directions">5. Project Progress and Future Directions</h3> <p><em>Completed Tasks</em>:</p> <ol> <li> <strong>AI Model Development</strong>: Successfully trained models to classify human activities based on EEG signals.</li> <li> <strong>Robotic Arm Design</strong>: Designed a functional 3-DOF robotic arm with simulated controls.</li> <li> <strong>Virtual Simulation</strong>: Validated AI-robotic arm interactions in a virtual environment.</li> </ol> <p><em>Future Directions</em>:</p> <ol> <li> <strong>Hardware Integration</strong>: Implement the developed AI models into physical robotic hardware for real-world testing.</li> <li> <strong>Real-Time EEG Acquisition</strong>: Develop a system for <strong>real-time EEG data acquisition</strong> and activity classification.</li> <li> <strong>Tactile Feedback System</strong>: Integrate tactile sensors with the robotic arm for <strong>real-world sensory feedback</strong>, complemented by Simulink-based simulations.</li> </ol> <hr> <h2 id="protocols">Protocols</h2> <p>Here is the protocol(steps) to reproduce our work with ease.</p> <iframe src="https://www.protocols.io/widgets/doi?uri=dx.doi.org/10.17504/protocols.io.n92ldr869g5b/v1" style="width: 100%; height: 300px; border: 1px solid transparent;"></iframe> <div class="container"> <object data="/assets/pdf/protocol_mcba.pdf" class="responsive-iframe" type="application/pdf"></object> </div> <hr> </body></html>